// Imports
import { DataLoader } from "./data-loader.js";
import { SentenceGrouper, groupTokensIntoSentences } from "./sentence-grouper.js";
import { UIHandler } from "./ui-handler.js";
import { buildVocab } from './tokenizer.js';
import { trainLanguageModel } from './trainer.js';
import { loadTrainedModel } from './loadModel.js';
import { Predictor } from './predictor.js';

async function main() {

    // üí° Zentrale Parameterdefinition
    const maxLen = 30;
    const embeddingDim = 64;
    const lstmUnits = 256;
    const epochs = 100;
    const batchSize = 32;

    // Initialisiere Klassen
    const loader = new DataLoader();
    const grouper = new SentenceGrouper();

    // Lade Daten
    const { trainData } = await loader.loadAll();

    // Gruppiere Tokens zu S√§tzen
    const tokenGroups = groupTokensIntoSentences(trainData.map(d => d.token));
    analyzeSentenceLengths(tokenGroups);
    console.log("Token-Gruppen (S√§tze):", tokenGroups.slice(0, 3));

    // Vokabular aufbauen
    const vocab = buildVocab(tokenGroups);
    const unkCount = Object.entries(vocab).filter(([word, id]) => id === vocab["<UNK>"]).length;
    console.log("Vokabulargr√∂√üe:", Object.keys(vocab).length);
    console.log("UNK-Zuweisungen im Vokabular:", unkCount);

    // ‚è¨ Versuche, ein bereits trainiertes Modell zu laden
    let model;
    try {
        model = await loadTrainedModel('trained-lm.json');
        console.log("‚úÖ Vortrainiertes Modell geladen.");
    } catch (e) {
        console.warn("‚ö†Ô∏è Kein gespeichertes Modell gefunden. Du kannst eines trainieren.");
    }

    // Predictor instanziieren (ggf. mit Dummy-Modell ‚Äì sp√§ter ersetzt)
    const predictor = new Predictor(model, vocab, maxLen);

    // UI initialisieren
    const ui = new UIHandler(predictor);
    ui.init();

    // Training-Button
    document.getElementById('train-btn').addEventListener('click', async () => {
        const subset = tokenGroups.slice(0, 1000); // Trainingsdaten
        //.slice(0, 5000)
        // ‚è¨ Training starten mit konsistenten Parametern
        const trainedModel = await trainLanguageModel({
            tokenGroups: subset,
            vocab,
            maxLen,
            embeddingDim,
            lstmUnits,
            epochs,
            batchSize,
        });

        // Predictor aktualisieren
        predictor.setModel(trainedModel, vocab, maxLen);

        console.log("‚úÖ Modelltraining abgeschlossen. Du kannst jetzt Texteingaben machen.");
    });
}

function analyzeSentenceLengths(tokenGroups) {
    const lengths = tokenGroups.map(s => s.length);
    const avg = (lengths.reduce((a, b) => a + b, 0) / lengths.length).toFixed(2);
    const median = lengths.sort((a, b) => a - b)[Math.floor(lengths.length / 2)];
    const shortCount = lengths.filter(l => l < 10).length;
    const longCount = lengths.filter(l => l > 50).length;
    const max = Math.max(...lengths);

    console.log("üìè Neue Satzl√§ngen-Analyse:");
    console.log(`‚û°Ô∏è Durchschnitt: ${avg} Tokens`);
    console.log(`‚û°Ô∏è Median: ${median}`);
    console.log(`‚û°Ô∏è Maximum: ${max}`);
    console.log(`‚û°Ô∏è < 10 Tokens: ${shortCount} S√§tze`);
    console.log(`‚û°Ô∏è > 50 Tokens: ${longCount} S√§tze`);
}


main();
