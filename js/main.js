
// Imports
import { DataLoader } from "./data-loader.js";
import { SentenceGrouper, groupTokensIntoSentences } from "./sentence-grouper.js";
import { DummyPredictor } from "./predictor.js";
import { UIHandler } from "./ui-handler.js";
import { buildVocab, tokensToSequences, padSequences } from './tokenizer.js';
import { prepareLanguageModelData} from './lm-preprocessing.js';



async function main() {
    // Initialisiere Klassen
    const loader = new DataLoader();
    const grouper = new SentenceGrouper();
    const predictor = new DummyPredictor();
    const ui = new UIHandler(predictor);

    // Lade Daten
    const { trainData, devData, testData } = await loader.loadAll();

    // Gruppiere Tokens zu SÃ¤tzen
    const tokenGroups = groupTokensIntoSentences(trainData.map(d => d.token));
    console.log("Token-Gruppen (SÃ¤tze):", tokenGroups.slice(0, 3));

    // ðŸ‘‰ Vokabular aufbauen und Tokens in IDs umwandeln
    const vocab = buildVocab(tokenGroups);
    const tokenSequences = tokensToSequences(tokenGroups, vocab);
    // Padding auf eine einheitliche LÃ¤nge, z.â€¯B. 50 Tokens
    const maxSeqLen = 50;
    const paddedTokenIdGroups = padSequences(tokenSequences, maxSeqLen);

    console.log("Vokabular-GrÃ¶ÃŸe:", Object.keys(vocab).length);
    console.log("Token-IDs (erste SÃ¤tze):", tokenSequences.slice(0, 2));
    console.log("Beispiel gepaddete Sequenz:", paddedTokenIdGroups[0]);

    // tokenIds: Array von Arrays mit Token-IDs pro Satz
    const { X, y } = prepareLanguageModelData(tokenSequences, Object.keys(vocab).length);

    const maxLen = 20; // z.â€¯B. maximale EingabelÃ¤nge
    const X_padded = padSequences(X, maxLen);

    console.log("Beispiel Eingabe (padded):", X_padded.slice(0, 3));
    console.log("ZielwÃ¶rter:", y.slice(0, 3));

    // Initialisiere BenutzeroberflÃ¤che
    ui.init();
}

main();
